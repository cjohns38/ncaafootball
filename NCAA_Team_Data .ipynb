{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, urllib, datetime, io\n",
    "import pandas as pd\n",
    "from scrapy import Spider, Request, signals\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.http.cookies import CookieJar\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSpider(Spider): \n",
    "    name = \"Data\"\n",
    "    \n",
    "    # Settings \n",
    "    custom_settings = {'DOWNLOAD_DELAY': '1.5'}\n",
    "    \n",
    "    ts=datetime.datetime.now()\n",
    "    \n",
    "    # Regex patterns \n",
    "    patterns = {'people' : \".*/\\d+\\?sport_code\\=MFB\",\n",
    "                'history' : \".*\\/teams\\/history\\/MFB\\/\\d+\",\n",
    "                'roster' : \".*\\/team\\/\\d+\\/roster\\/\\d+\",\n",
    "                'stats' : \".*\\/team\\/\\d+\\/stats\\/\\d+\",\n",
    "               }\n",
    "    \n",
    "    # Files \n",
    "    _files = {'people':{'file':\"People_{0}.csv\".format(ts), \n",
    "                        'header':\"team,person,link,\\n\",\n",
    "                       }, \n",
    "              'history':{'file':\"History_{0}.csv\".format(ts),\n",
    "                         'header':\"team,link,\\n\",\n",
    "                        }, \n",
    "              'roster':{'file':\"Roster_{0}.csv\".format(ts), \n",
    "                        'header':\"team,link,\\n\",\n",
    "                       }, \n",
    "              'stats':{'file':\"Stats_{0}.csv\".format(ts), \n",
    "                       'header':\"team,link,\\n\"\n",
    "                      },\n",
    "             }\n",
    "    \n",
    "    newLinksFile = \"NewLinks{0}.csv\".format(ts)\n",
    "    \n",
    "    with open(newLinksFile, 'w') as f: \n",
    "        f.write('team,link,txt,key,match\\n')\n",
    "        \n",
    "    # Table results \n",
    "    with open(\"Results.csv\", \"w\") as f: \n",
    "        f.write(\"Team,Date,Opponent,Result\\n\")\n",
    "                \n",
    "    # Table Team stats \n",
    "    with open(\"TeamStats.csv\", \"w\") as f: \n",
    "        f.write(\"Team,Stat,Rank,Value,Year\\n\")\n",
    "                \n",
    "    # Individual \n",
    "    with open(\"Individual.csv\", \"w\") as f: \n",
    "        f.write(\"Team,Stat,Player,Value,Year\\n\")\n",
    "                \n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler, *args, **kwargs):\n",
    "        spider = super(DataSpider, cls).from_crawler(crawler, *args, **kwargs)\n",
    "        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)\n",
    "        return spider\n",
    "    \n",
    "    def start_requests(self): \n",
    "        links = pd.read_csv(\"Teams_2017-12-02 14:55:51.037193.csv\")\n",
    "        links = links.to_dict(orient='record')\n",
    "        \n",
    "        for link in links[:1]: \n",
    "            yield Request(url=link['link'], \n",
    "                          callback=self.parse,\n",
    "                          meta= {'REDIRECT_ENABLED': True, 'year':link['year']}\n",
    "                         )\n",
    "    \n",
    "    def parse(self, response): \n",
    "        # Team \n",
    "        team = response.selector.xpath(\"//body//div//fieldset//legend//a/text()\").extract()[0]\n",
    "        print(team)\n",
    "        # Get all the links on the page\n",
    "        le = LinkExtractor() \n",
    "        links = le.extract_links(response)\n",
    "                \n",
    "        # Extract the links and save to a list\n",
    "        linksOut = []\n",
    "        for link in links: \n",
    "            for k, pattern in self.patterns.items():\n",
    "                if re.search(pattern, link.url) != None: \n",
    "                    linksOut.append({'team':team, \n",
    "                                     'link':link.url, \n",
    "                                     'txt':link.text, \n",
    "                                     'key':k, \n",
    "                                     'match':True, \n",
    "                                     'year':response.meta['year']\n",
    "                                    }\n",
    "                                   )\n",
    "        \n",
    "        # Tables \n",
    "        tables = pd.read_html(response.body)\n",
    "        \n",
    "        # Results \n",
    "        tables[1].rename(columns=tables[1].iloc[1], inplace = True)\n",
    "        tables[1].drop([0,1], inplace = True)\n",
    "        tables[1]['team'] = team\n",
    "                \n",
    "        # Team stats \n",
    "        tables[2].rename(columns=tables[2].iloc[1], inplace = True)\n",
    "        tables[2].drop([0,1], inplace = True)\n",
    "        tables[2]['team'] = team\n",
    "        tables[2]['year'] = response.meta['year']\n",
    "                \n",
    "        # Individual stats\n",
    "        tables[3].rename(columns=tables[3].iloc[1], inplace = True)\n",
    "        tables[3].drop([0,1], inplace = True)\n",
    "        tables[3]['team'] = team\n",
    "        tables[3]['year'] = response.meta['year']\n",
    "\n",
    "        # Create a pandas df and append to csv \n",
    "        c = io.StringIO()\n",
    "        df = pd.DataFrame(linksOut)\n",
    "        df[['team', 'link', 'txt', 'key', 'match']].to_csv(c, index=False, header=False)\n",
    "                \n",
    "        io_results = io.StringIO()\n",
    "        io_teamstats = io.StringIO()\n",
    "        io_individual = io.StringIO()\n",
    "                \n",
    "        tables[1][['team', 'Date','Opponent','Result']].to_csv(io_results, index=False, header=False)\n",
    "        tables[2][['team','Stat','Rank','Value', 'year']][:-1].to_csv(io_teamstats, index=False, header=False)\n",
    "        tables[3][['team','Stat','Player','Value', 'year']][:-1].to_csv(io_individual, index=False, header=False)\n",
    "        \n",
    "        # Append  \n",
    "        with open(self.newLinksFile, 'a') as f:\n",
    "            f.write(c.getvalue())\n",
    "        \n",
    "        with open('Results.csv', 'a') as f: \n",
    "            f.write(io_results.getvalue())\n",
    "            \n",
    "        with open(\"TeamStats.csv\", 'a') as f: \n",
    "            f.write(io_teamstats.getvalue())\n",
    "        \n",
    "        with open(\"Individual.csv\", 'a') as f: \n",
    "            f.write(io_individual.getvalue())\n",
    "        \n",
    "    def spider_closed(self, spider): \n",
    "        print(\"Spider is done\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-06 21:24:15 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n",
      "2017-12-06 21:24:15 [scrapy.utils.log] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36'}\n",
      "2017-12-06 21:24:15 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2017-12-06 21:24:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-12-06 21:24:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-12-06 21:24:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-12-06 21:24:15 [scrapy.core.engine] INFO: Spider opened\n",
      "2017-12-06 21:24:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-12-06 21:24:15 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-12-06 21:24:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://stats.ncaa.org/teams/62793> from <GET http://stats.ncaa.org/team/721/11520>\n",
      "2017-12-06 21:24:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://stats.ncaa.org/teams/62793> (referer: None)\n",
      "2017-12-06 21:24:21 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2017-12-06 21:24:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 591,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 7562,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 12, 7, 3, 24, 21, 806117),\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 7,\n",
      " 'memusage/max': 120852480,\n",
      " 'memusage/startup': 120852480,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2017, 12, 7, 3, 24, 15, 660315)}\n",
      "2017-12-06 21:24:21 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air Force Falcons\n",
      "Spider is done\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36'\n",
    "})\n",
    "\n",
    "process.crawl(DataSpider)\n",
    "process.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
